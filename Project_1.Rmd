---
title: "Project-01: SEMMA with Regularized Logistic Regression"
author:
- Md Al Masum Bhuiyan
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{STAT 5494-- Statistical Data Mining}
- \lhead{SEMMA with Regularized Logistic Regression}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}
geometry: margin=1in
spacing: single
fontsize: 11pt
---
\newpage
\section{Problem-1: Data Collection}
We obtain the data directly in R using mlbench package and save the data in a CSV file. The code is as follows:
```{r}
#install.packages("mlbench")
data(BreastCancer, package="mlbench")
data <-BreastCancer
write.csv(data, file="BreastCancer.csv", row.names =FALSE)
```


\section{Problem-2: Exploratory Data Analysis}
To perfrom exploratory data analysis,  we first inspect the data types, quantity and percentage of zeros, infinte numbers, missing, and unique values of data using $df\_status$ function:
```{r, message=F, warning=F}
#install.packages("funModeling")
library("funModeling")
dim(data)
head(data)
#str(data)
df_status(data) # Getting the metrics about data types, zeros, infinite numbers, and missing values:
```


\subsection{Removing column ID}
We prepare the data by removing column ID, Since it will not affect in our analysis.
```{r}
dat <- data[, -1]
dim(dat)
```



\subsection{Inspecting the distinct values of each variable}
We inspect the distinct values of each variable of data. The following code shows that the traget Class has two categorical levels: 458 benign and 241 malignant. The other variable is also shown 10 levels with number of values.
```{r}
for (j in 1:NCOL(dat)){
  print(colnames(dat)[j])
  print(table(dat[,j], useNA="ifany"))
}
```

\subsection{Visualizing Missing values} 
We now visualize the quantity, percentage of missing values for each variables. We see that only the Bare.nuclei variable has 16 NA values, which is 2.29 percent of the data. We also visualize the  the missing values for categorical variables and then we omit the NA data.
```{r, message=F, warning=F}
#install.packages("naniar")
library(naniar)
vis_miss(dat)
gg_miss_var(dat)
##Visulaizing the missing values for target Class of each variables
gg_miss_fct(x = dat, fct = Class)
# remove rows containing missing values
dat <- na.omit(dat)
dim(dat)
```

\newpage
\subsection{Frequency Distribution of the Target variable Class}
The following code shows the frequency, percentage, cumulative percentage of the target variable Class.
```{r}
freq(dat$Class)
```

To analyze the data, we make numeric values 1 and 0 for categorical variable Class.
```{r}
dat$y <- ifelse(dat$Class=="benign", 1, 0)
dim(dat)
```

\subsection{Inspecting the outlying records}
We now plot a Box plot to inspect the outlying records for each variable. As we see in the graph, $Marg\_adhesion$, $Epith.c.size$, $Bl\_cromatin$, $Normal\_Nucleoli$ has few outlying value. $Mitoses$ variable hase most of the outlying values.
```{r}
data_box <- dat[, -11:-10] # Removing Class variable (Bcz of many variables)
boxplot(data_box, main = "Boxplot", horizontal = F, col="orange",
        border="brown")
```

To analyze the data, we change the character variable of into numeric variable.
```{r}
dat <- dat[, c(1:9, 11)]
dat <- apply(dat, 2, FUN=function(x) {as.numeric(as.character(x))})
dat <-na.omit(dat)
dat <- as.data.frame(dat)
```



\subsection{Association using $Chi^2$ and Fisher test}
To check the assoication between Class and other attributes, we use $Chi^2$ and fisher test. Our hypotheses are as follows:

$H_0$: The two variables are independent,

$H_1$: The two variables are dependent.

since the p-value is less than the significance level (0.05) for all cases (between class and other attributes), we reject the null hypothesis and conclude that the Class and other attribute are dependent to each other. The code and result are as follows:


```{r, message=F, warning=F}
#============================================================
# Chi-sq and Fisfter test for Class and Other varibales
#============================================================
dat1 <- dat[, -11]
chitest <- matrix (0, 9, 4)
ftest <- matrix (0, 9, 2)

for (j in 1: (ncol(dat1)-1)){
  testor <- table(as.vector(dat1 [, ncol(dat1)]), as.numeric(dat1[, j]))
  chi2 <- chisq.test(testor, correct=FALSE)
  chitest[j, ] <- c(colnames(dat1)[j], round(chi2$statistic, digits = 2), chi2$p.value,chi2$parameter)
  s = fisher.test(testor, simulate.p.value = TRUE, B=1e5)
  ftest[j, ] <- c(colnames(dat1)[j], s$p.value)
}

colnames(chitest) <- c("Names", "Statistics", "p-values", "D.Freedom")
colnames(ftest) <- c("Names", "p-values")
names(dimnames(chitest)) <- list("", "Association among Class and other predictors Using Chi test \n")
names(dimnames(ftest)) <- list("", "Association among Class and other predictors Using Fisher test \n")
chitest
ftest
```

\subsubsection{Measure the association between Class and other variables}
In this subsection, we study the assoiciation plot (given below) where the diagonal element K refers to number of unique levels for each variable. This measure of association indicates the strength of the relationship, whether, weak or strong. The off-diagonal elements contain the forward and backward $\tau$ measures for each variable pair. Specifically, the numerical values appearing in each row represent the association measure $\tau(x,y)\tau(x,y)$ from the variable xx indicated in the row name to the variable yy indicated in the column name.


For example, the variable Cell.size is almost perfectly predictable (i.e. $\tau(x,y)=0.78)$ from Class and this forward association is quite strong. The forward association suggest that x=Cell.size is highly predictive of y=Class. It indicates that if we know a Cell.size, then we can easily predict its Class.


On the contrary, the reverse association y=class and x= Cell.size (i.e. $\tau(y,x)=0.27)$; is a strong association and indicates that if we know the Class then its easy to predict its Cell.size.


From chi-squared and Fisher significance test, we have found Normal.Neocleoli and Cell.thickness are dependent to each other. But forward and reverse association plot suggest that x=Normal.neocleuli shape is weakly associated to y= Cell.thickness (i.e.$\tau (x,y)=0.17$) and (i.e.$\tau(y,x)=0.060$). So we conclude that although these two variables are significant but their association is weak; i.e. it will be difficult to predict one from another.

```{r,message=F, warning=F}
library(GoodmanKruskal)
varset1<- c("Cl.thickness", "Cell.size", "Cell.shape",  "Marg.adhesion",  "Epith.c.size", "Bare.nuclei", "Bl.cromatin",  "Normal.nucleoli", "Mitoses", "Class")
associate1<- subset(data, select = varset1)
GKmatrix1<- GKtauDataframe(associate1)
plot(GKmatrix1, corrColors = "blue")
```
\newpage
\section{Problem-3: Data Partition}
In this section, I partition the data into three parts, the training data D1, the validation data D2, and the test data D3, with a ratio of $2:1:1$.


```{r}
set.seed(123)
n <- nrow(dat)
id.split <- sample(x=1:3, size = n, replace =TRUE, prob=c(0.5, 0.25, 0.25))
dat.train <- dat[id.split ==1, ]
dat.valid <- dat[id.split ==2, ]
dat.test <- dat[id.split == 3, ]
```


\section{Problem 4:}
\subsection{Problem-4(a): Builiding a Logistic Rgression Model}

Here, I fit the regularized logistic regression using the training data D1 with Lasso model. In glmnet function, the family argument specify that we want a “binomial” model which tells glmnet() to fit a logistic function to the data.


```{r,message=F, warning=F}
#install.packages("glmnet")
library(glmnet)
formula0 <- y~Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + Mitoses
X <- model.matrix (as.formula(formula0), data = dat.train)
y <- dat.train$y

```


\subsection{Problem-4(b): Selecting the best tuning parameter}
Next I would like to see how the model is doing when predicting Class(y) on data. I used $pred$ function in the form of P(y=1|X) using parameter $type='response'$ which tells predict to return probabilities. The decision boundary will be 0.5. If P(y=1|X) > 0.5 then y = 1 (benign) otherwise y=0 (malignant). Therefore I used misclassification rate and the mean square error (mse) for the predicted probabilities.

I select the best tuning parameter using the validation data D2 and choosing the minimum mean squared error(mse).
```{r}
X.valid <- model.matrix (as.formula(formula0), data = dat.valid)
y.valid <- dat.valid$y

Lambda <- seq(0.0001, 0.5, length.out = 200)
L <- length(Lambda)
OUT <- matrix (0, L, 3)
for (i in 1:L){
  fit <- glmnet(x=X, y=y, family ="binomial", alpha =1, #lasso
                lambda=Lambda[i], standardize=T, thresh = 1e-07, maxit=1000)
  pred <- predict(fit, newx=X.valid, s=Lambda[i], type="response")
  miss.rate <- mean(y.valid != (pred > 0.5))
  mse <- mean((y.valid - pred)^2)
  OUT[i, ] <- c(Lambda[i], miss.rate, mse)
  
}
head(OUT)
par(mfrow = c(1,2))
plot(OUT[, 1], OUT[,2], type = "b", col = "blue", ylab = "Missclassification rate")
plot(OUT[, 1], OUT[,3], type = "b", col = "red", ylab = "MSE")
(lambda.best <- OUT[which.min(OUT[, 3]), 1])
(miss.rate_Lambda <- OUT[which.min(OUT[, 3]), 2])

```
The best fitted lambda is 0.00261206, where the corresponding misclassification rate is  is 0.02840909. So I conclude that the accuracy on this model is good.
 
 
 
\subsection{Problem-4(c): Final 'best' model by pooling D1 and D2.}
I then present the final ‘best’ model fit by pooling D1 and D2 together. Using the beta from fit.best model, I see the coefficients of the model and select the important predictors.

```{r}
X.12 = rbind(X, X.valid)
y.12 = c(y, y.valid)
fit.best <- glmnet (x=X.12, y=y.12, family ="binomial", alpha=1,  #LASSO
        lambda = lambda.best, standardize = T, thresh = 1e-07, maxit=1000)
names(fit.best)
fit.best$beta # Finding important variables.
```
Since I do not get any coefficients for Cell.size, it is not an important predictor for the predective model. The important predictors are Cl.thickness, Cell.shape, Marg.adhesion, Epith.c.size ,Bare.nuclei, Bl.cromatin, Normal.nucleoli, and Mitoses.

\section{Problem-5}
\subsection{Final logistic model to the test data D3}
Using test data D3, I apply the final logistic model.
```{r}
X.test <- model.matrix(object=~Cl.thickness + Cell.size +Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli+ Mitoses, data=dat.test)
pred <- predict(fit.best, newx = X.test, s =lambda.best, type="response")
dim(pred)
```

\subsection{ROC curve and AUC}
ROC suggests the accuracy of a classification model at a threshold value. It determines the model's accuracy using Area Under Curve (AUC). The AUC also referred to as index of accuracy (A) or concordant index (ci), represents the performance of the ROC curve. The idea is that higher the area, better the model.

```{r, message=F, warning=F}
library(cvAUC)
yobs <- dat.test$y
AUC <- ci.cvAUC(predictions = pred, labels =yobs, folds=1:NROW(dat.test), confidence = 0.95)
AUC
auc.ci <- round(AUC$ci, digits = 3)

library(verification)
mod.glm <- verify(obs = yobs, pred = pred)
roc.plot(mod.glm, plot.thres=NULL)
text(x=0.7, y=0.2, paste("Area under ROC = ", round(AUC$cvAUC, digits = 3), "with 95% CI (",
                         auc.ci[1], ",", auc.ci[2], ").", sep = " "), col="blue", cex =1.2)

confusionMatrix(pred, yobs)
```


ROC is plotted between True Positive Rate (Y axis) and False Positive Rate (X axis). In this plot, our aim is to push the curve (shown below) toward 1 (left corner) and maximize the area under curve. The diagonal line represents the ROC curve at 0.5 threshold. 
Since AUC is 0.99 (closer to 1) and the curve almost appraoches to 1, we can say that the model have good predictive ability.