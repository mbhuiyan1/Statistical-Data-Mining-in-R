---
title: "Project-II"
author:
- Md Al Masum Bhuiyan
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{STAT 5494-- Statistical Data Mining}
- \lhead{Project II}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}
geometry: margin=1in
spacing: single
fontsize: 11pt
---
\newpage
\section{Data collection}
We bring the data from UCI Machine Learning Repository.
```{r, message=F, warning=F}
train <- read.table(file="http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra",sep=",", header = FALSE, na.strings = c("NA", "", " "), 
                    col.names = c(paste("x", 1:64, sep=""), "digit"))
dim(train)
```
We see that we have 64 measurements for a total of 3823 handwritten digits. The 65th $``$measurement$"$ is not actually a measurement, but rather the actual (true) digit given the values from the 64 other measurements.


\subsection{Examine the data briefly}
We next explore the data by detecting the missing values as follows:

\subsubsection{Detecting missing values}
```{r, message=F, warning=F}
#install.packages("VIM")
library(VIM)
aggr(train, col=c('navyblue','yellow'), numbers=TRUE, sortVars=TRUE,
    labels=names(train), cex.axis=.7, gap=3, ylab=c("Missing%","Pattern"))
```
There is no missing values in the trained data.
\subsubsection{Detecting NA values}
```{r}
any(is.na(train))  #Checking for NAs
```
There is no NA value in the trained data.
\subsubsection{Qualitative analysis}
Now we perform the qualitative anaysis to examine the data briefly.
```{r, message=F, warning=F}
# ======================  quntitative analysis=============
library("funModeling")
df_status(train)
```

\subsubsection{inspecting the distinct values}
We ommitted the results due to large size of  outputs.
```{r, results=FALSE}
#====================== inspecting the distinct values=======
for (j in 1:NCOL(train)){
  print(colnames(train)[j])
  print(table(train[,j], useNA="ifany"))
}
```

\subsection{Detect unary column}
I next order train data based on train$digit (the true digit). I place the true digits into a vector to be used later for labeling, remove the known digit in the 65th column. We remove the measurements that have uniary column, as they contribute no additional information and thus their inclusion is not necessary for analysis.

```{r}
dat0 <- data.matrix(train[order(train$digit),])
labs <- dat0[, c(65)] # Labels for plotting needed later
dat0 <- dat0[,-c(65)] # Remove the known digits
n <- NROW(dat0)
color <- rainbow(n, alpha=0.8)
heatmap(dat0, col=color, scale="column", Rowv=NA, Colv=NA,
        labRow=FALSE, margins=c(4,4), xlab="Image variables", ylab="Samples", main="Handwritten digit train data")
```

Before computing the number of unary column, we first see the heat map of train data. From the heatmap, it is clear that there are no observations recorded for the 1st and 40th variable of train data. 
```{r}
uniq <- apply(dat0, 2, unique)
  for (k in 1:length(uniq)){
    if (length(unique(dat0[,k])) == 1) 
      print(paste("x", k))
  }
```

So the 1st and 40 th columns are unary in train data. I remove the unary colums and I have measurements for each of the variables for which the value recorded in the region observed is not constant. I also remove the 33th column, which has been discussed in the testing data section. I then plot these values with heatmap() again to identify any potential patterns. 
```{r}
dat0 <- dat0[, -c(1, 33, 40)]
n <- NROW(dat0)
color <- rainbow(n, alpha=0.8)
heatmap(dat0, col=color, scale="column", Rowv=NA, Colv=NA,
        labRow=FALSE, margins=c(4,4), xlab="Image variables", ylab="Samples", main="Handwritten digit train data")
```

While not initially visible, there seems to be approximately ten distinct regions when looking at the plot in heatmap figure. This makes sense as the values were sorted prior to plotting, and we are looking at the handwritten digits of $0, 1,\cdot \cdot \cdot 9$.


\section{Problme 1(b)} 
I excluded the target variable in the previous subsection. Now I begin to perform PCA with the data. I first scale the data, and then compute the Principal Components (PCs) for the train data.
```{r, message=F, warning=F}
library(RColorBrewer)
palette(brewer.pal(n=length(unique(labs)), name="Set3"))
dat0.scaled <- data.frame(apply(dat0, 2, scale)) 
```

\subsection{Principal Component Analysis (PCA)}
```{r}
pca.res <- prcomp(dat0.scaled, retx=TRUE)
sd.pc <- pca.res$sdev
var.pc <- sd.pc**2
```

Next, I construct a scree plot showing the cumulative proportions of variation for all of the PCs and I choose two eigenvalues.
```{r}
par(mfrow=c(1,3), mar=rep(4,4))
plot(pca.res)
screeplot(pca.res, type="lines", main="Scree Plot")
plot(var.pc, type="h", xlab="Principal Component (PC)",
     ylab="Variances", main="Plot of PC vs Variance") 

```

Now I plot PC2 versus PC1, where the ‘dots’ for each digit are represented with different colors and digit symbols of training data.
```{r}
par(mfrow=c(1,1), mar=rep(4,4))
plot(pca.res$x[,1:2], pch="", main="PC.1 and PC.2 for Hand digits for Training data")
text(pca.res$x[,1:2], labels=labs, col=labs)
abline(v=0, lty=2)
abline(h=0, lty=2)
```

In the plot we see that the similar digits are closer to one another and make a cluster. Sometimes they overlap with the same or different digits.

Here we plot the proprotion of variance and cumulative proportion of variance with their prinicipal components. After that we show the PARETO plot to put variances and cumulative variances together. 
```{r}
prop.pc <- var.pc/sum(var.pc)
plot(prop.pc, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", type = "b")

plot(cumsum(prop.pc), xlab = "Principal Component", col="blue",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b", pch=19) # for cumulative
#The Pareto plot
par(mar=c(4, 4, 4, 4))
bar <- barplot(var.pc, ylab="Variance Explained", col="skyblue", 
               xlab="# Princinpal Components", col.axis="blue", col.lab="blue")
mtext(1:length(var.pc),side=1, line=1,at=bar,col="black")
par(new=T)
plot(bar, cumsum(prop.pc),axes=F,xlab="", ylab="", col="orange", type="b", 
     col.lab="orange", col.lab="orange")
axis(4,col="orange", col.ticks="orange", col.axis="orange")
abline(h=.90, lty=2, col="green", lwd=.8)
mtext("Cumulative Proportion of Variance Explained",side=4,line=3,col="orange")
title(main = 'Pareto Chart from PCA')
```

\newpage
\section{Problme 1(c)}
 Now we analyze the Kernel PC for hand digits  training data.
 
\subsection{Kernel PCA}
```{r}
library(kernlab)
kpc <- kpca(~., data=dat0.scaled, kernel="rbfdot", kpar=list(sigma=0.0001), features=2)
eig(kpc)        # returns the eigenvalues
kernelf(kpc)    # returns the kernel used when kpca was performed
PCV <- pcv(kpc) # returns the principal component vectors
dim(PCV)
head(PCV)
PC <- rotated(kpc)    # returns the data projected in the (kernel) pca space
head(PC);
```
Now we plot the data projections for two KPC of digits train data.

```{r}
plot(PC, pch="", xlab="1st Kernel PC", ylab="2nd Kernel PC", main="Kernel PCA")
text(PC, labels=labs, col=labs)

```

Here we compute noncumulative and cumulative proportions of variation explained. And, we show the pareto plot to put variances and cumulative variances together.
```{r}
var.pc <- eig(kpc)
names(var.pc) <- 1:length(var.pc)
prop.pc <- var.pc/sum(var.pc)

par(mfrow=c(1,2), mar=rep(4,4))
# NONCUMULATIVE
plot(prop.pc, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", type = "b")
# CUMULATIVE
plot(cumsum(prop.pc), xlab = "Principal Component", col="blue",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b", pch=19)
#THE PARETO PLOT
par(mar=c(4, 4, 4, 4), mfrow=c(1,1))
bar <- barplot(var.pc, ylab="Variance Explained", col="cadetblue3", 
               xlab="Princinpal Components", col.axis="gray45", col.lab="gray45")
mtext(1:length(var.pc),side=1, line=1,at=bar,col="black")
par(new=T)
plot(bar, cumsum(prop.pc),axes=F,xlab="", ylab="", col="chocolate", type="b", 
     col.lab="orange", col.lab="orange", lwd=2)
axis(4, col="chocolate2", col.ticks="orange", col.axis="chocolate2")
abline(h=.90, lty=2, col="lightgoldenrod4", lwd=2)
mtext("Cumulative Proportion of Variance Explained", side=4, line=3,col="chocolate2")
title(main = 'Pareto Chart from Kernel PCA', cex.main=1.5)
```


\subsection{Comparison}
I then compare the oridnary PC and kernel PC as follows:

```{r}
par(mfrow=c(1,2), mar=rep(4,4))
plot(pca.res$x[,1:2], pch="", main=" Ordinary PCA")
text(pca.res$x[,1:2], labels=labs, col=labs)
abline(v=0, lty=2)
abline(h=0, lty=2)

plot(PC, pch="", xlab="1st Kernel PC", ylab="2nd Kernel PC", main="Kernel PCA")
text(PC, labels=labs, col=labs)
```

From these plots, we notice that the Kernel PCA provides good representative direction. Clusters are more visible in the case of Kernal PCA than the oridinary PCA. Digits are more overlapping in Kernel PCA plot than the ordinary plot. 

\subsection{Problem 1(d)}
\subsubsection{Test data collection}
```{r}
# ==================================================================
test <- read.table(file="http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes",sep=",", header = FALSE, na.strings = c("NA", "", " "), col.names = c(paste("x", 1:64, sep=""), "digit"))

```

\subsubsection{Detecting missing values for test data}

```{r, message=F, warning=F}
#install.packages("VIM")
library(VIM)
aggr(test, col=c('navyblue','yellow'), numbers=TRUE, sortVars=TRUE,
    labels=names(train), cex.axis=.7, gap=3, ylab=c("Missing%","Pattern"))
```

```{r}
any(is.na(test))  #Checking for NAs
```
There are no missing values in the test data.


Since we would use the predict function for test data, so we remove the knonw digits column (65) and column (1, 33 and 40) to make same dimension of train data. Here, 33th colmun has zero values, so it is better to remove for scaling the testing data.

```{r}
dat1 <- data.matrix(test[order(test$digit),])
labs <- dat1[, c(65)] # Labels for plotting needed later
table(test$x33)
dat1 <- dat1[, -c(1, 40, 33, 65)]
n <- NROW(dat1)
color <- rainbow(n, alpha=0.8)
heatmap(dat1, col=color, scale="column", Rowv=NA, Colv=NA,
        labRow=FALSE, margins=c(4,4), xlab="Image variables", ylab="Samples", main="Handwritten digit Test data")
```

Now we scale the test data before prediction.
```{r}
test.scaled <- data.frame(apply(dat1, 2, scale)) 
pred_pca <- predict(pca.res, test.scaled)
```

Now we  plot PC2 versus PC1, where the ‘dots’ for each digit are represented with different colors and digit symbols of testing data.
```{r}
par(mfrow=c(1,2), mar=rep(4,4))
plot(pca.res$x[,1:2], pch="", main=" PC (Training data)")
text(pca.res$x[,1:2], labels=labs, col=labs)
abline(v=0, lty=2)
abline(h=0, lty=2)
#======================================================
plot(pred_pca[,1:2], pch="", main="PC (Testing data)")
text(pred_pca[,1:2], labels=labs, col=labs)
abline(v=0, lty=2)
abline(h=0, lty=2)
```

The two plots  show the PCA between train data and test data. We conlclude that PCs of testing data are able to effecively predict the test data, since they show the more visible cluster than train data.


Now we plot KPC2 versus  KPC1, where the ‘dots’ for each digit are represented with different colors and digit symbols of testing data.
```{r}
pred_kpca <- predict(kpc, test.scaled)

```

```{r}
par(mfrow=c(1,2), mar=rep(4,4))
plot(PC, pch="", xlab="1st Kernel PC", ylab="2nd Kernel PC", main="Kernel PCA (Training data)")
text(PC, labels=labs, col=labs)
#==================================
plot(pred_kpca[,1:2], pch="", main="KPC.1 and KPC.2 (Testing data)")
text(pred_kpca[,1:2], labels=labs, col=labs)
abline(v=0, lty=2)
abline(h=0, lty=2)
```

We conlclude that KPCs of testing data are able to effecively predict the test data, since they show the more visible cluster than train data.


\section{Problem 2}
In this section, we analyze the association rules. 
\subsection{Problem 2(a)}
```{r, message=F, warning=F}
library(arules)
dat2 <- read.transactions(file="/Users/masum/Desktop/Fall_2018/Data_Mining/LAB/Lecture-5/AV1611Bible.txt", format = "basket", sep ="", rm.duplicates =F)
dim(dat2)
inspect(dat2[1:5, ])
```


\subsection{Problem 2(b)}
I plot the items with high frequencies and perform the frequent itemsets with support 0.08. 
```{r}
itemFrequencyPlot(dat2, support = 0.08, cex.names = 0.8, col="green")
```

Therefore, the item with the higest frequency is $``$lord$"$.

```{r}
# The top ten items
item.freq <- itemFrequency(dat2, type = "relative")
item.freq <- sort(item.freq, decreasing = TRUE)
item.freq[1:10]
```

I then Set up the parameters and function $``$apriori$"$ with choices support $0.001$ explore the resultant rules using summary function.
```{r}
(rules <- apriori(dat2, parameter = list(support = 0.01, confidence = 0.6, 
                                         target ="rules", maxlen=6)))
inspect(rules[1:12])
summary(rules)
```


\subsection{Problem 2(c)}
```{r}
RULES <- as(rules, "data.frame")
rules0 <- data.frame(matrix(unlist(strsplit(as.character(RULES$rules), split="=>")), ncol=2, byrow=TRUE))
colnames(rules0) <- c("LHS", "RHS")
rule.size <- function(x){length(unlist(strsplit(as.character(x), split=",")))}
rules0$size <- apply(rules0, 1, rule.size)
rules0$size[as.character(rules0$LHS)=="{} "] <- rules0$size[as.character(RULES$LHS)=="{} "]-1 
RULES <- cbind(RULES, rules0)
head(RULES,12)
```

I list the top 5 rules in decreasing order of confidence (conf) for item sets of size 2 and 3 which satisfy the support threshold 0.01.
```{r}
RULES1 <- RULES[RULES$size==2, ]
RULES1 <- RULES[ order(RULES$confidence, decreasing = TRUE), ]
head(RULES1,n=5)
```

We then perform frequent itemsets by listing the top 5 rules in decreasing order of the lift measure for item sets of size 2 and 3.
\subsection{Problem 2(d)}
```{r}
RULES1 <- RULES[RULES$size==2, ]
RULES1 <- RULES[ order(RULES$lift, decreasing = TRUE), ]
head(RULES1,n=5)
```

\subsection{Problem 2(e)}
Conviction actually captures the notion of an implication rules. Unlike lift, Conviction is sensitive to rule direction (i.e. $conv(A\rightarrow B) \neq conv(B\rightarrow A)$). It attempts to measure the degree of implication of a rule. Unlike Confidence, the support of the both antecedent and consequent are considered in conviction.

